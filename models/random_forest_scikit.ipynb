{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model For Predicting First Day IPO Performance\n",
    "\n",
    "This notebook trains and tests a random forest model to predict whether an IPO (Inital Public Offering) will be underpriced or not. Please refer to the [paper](./research-paper.pdf) for full documentation. \n",
    "\n",
    "To begin, we will import the necessary modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# visualizaitions\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "# utilities \n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the ipo data from a csv to a pandas data frame.\n",
    "\n",
    "After collecting the labels for the data points, we select all numeric features so that it can be processed in the random forest model. We store the labels of the features we choose to use in `ipo_features`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Features: ['Profit Margin', 'Return on Assets', 'Offer Size (M)', 'Shares Outstanding (M)', 'Offer Price', 'Market Cap at Offer (M)', 'Cash Flow per Share', 'Instit Owner (% Shares Out)', 'Instit Owner (Shares Held)', 'Real GDP Per Capita', 'OECD Leading Indicator', 'Interest Rate', 'Seasonally Adjusted Unemployment Rate', 'CPI Growth Rate', 'Industry Sector', 'Industry Group', 'Industry Subgroup']\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "ipos = pd.read_csv(\"../data/clean_bloomberg_with_sectors_macro.csv\")\n",
    "# get labels\n",
    "ipo_labels = ipos[\"Underpriced\"].tolist()\n",
    "# get features\n",
    "ipos = ipos.select_dtypes(['float64', 'float32', int])\n",
    "ipo_features = ipos._get_numeric_data().columns.values.tolist()[1:-1]\n",
    "# remove feature wich defines the label\n",
    "ipo_features.remove('Offer To 1st Close')\n",
    "# # TODO remove these features from the csv\n",
    "# ipo_features.remove('Shares Outstanding (M).1')\n",
    "# ipo_features.remove('Offer Size (M).1')\n",
    "print(\"Possible Features:\", ipo_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features we have chose to use are stored in `ipo_features_data` as a pandas dataframe. Using this data frame long with the labels, we make a test and training split. \n",
    "\n",
    "We then use `sci-kit learn`'s Random Forest model to initialize a classification model. This model is trained on the designated training data we have created. We then make a prediction by feeding the newly created model the test set we created.\n",
    "\n",
    "Random forest is an ensemble machine learning algorithm that uses multiple decision trees to make predictions. It works by randomly selecting a subset of features from the dataset and then building a decision tree for each subset. This is process is called bagging. Each tree is then used to make a prediction, and the final prediction is made by taking the average of all the individual tree predictions. Bagging along with with the other processes helps reduce overfitting and improves accuracy. Random forest also has the ability to handle large datasets with high dimensionality, making it a powerful tool for predictive analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleinin/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# get columns for specified features\n",
    "ipo_features = ipo_features[:-1]\n",
    "ipo_features_data = ipos[ipo_features]\n",
    "# split dataset to trianing set and test set\n",
    "ipo_features_data_train, ipo_features_data_test, ipo_labels_train, ipo_labels_test = train_test_split(ipo_features_data, ipo_labels, test_size=0.3)\n",
    "# create classifier \n",
    "clf = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "                             criterion=\"entropy\", max_depth=13, max_features=\"auto\", \n",
    "                             max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                             min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, \n",
    "                             n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
    "                             warm_start=False)\n",
    "\"\"\"\n",
    "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', \n",
    "'max_depth': 13, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, \n",
    "'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, \n",
    "'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
    "\"\"\"\n",
    "# train the model\n",
    "clf.fit(ipo_features_data_train, ipo_labels_train)\n",
    "# predict\n",
    "ipo_labels_pred = clf.predict(ipo_features_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise Acurracy: [0.18012422 0.89971347]\n",
      "Overall Accuracy: 0.6725490196078432\n"
     ]
    }
   ],
   "source": [
    "matrix = metrics.confusion_matrix(ipo_labels_test, ipo_labels_pred)\n",
    "print(\"Class-wise Acurracy:\", matrix.diagonal()/matrix.sum(axis=1))\n",
    "print(\"Overall Accuracy:\", metrics.accuracy_score(ipo_labels_test, ipo_labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279999999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.14 + 0.178 + 0.180) / 3\n",
    "#(0.946 + 0.939 + 0.899) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc31e280b75524b4b3dc48620d9955e9f0dcb074fec9ce8e5d6a0782dd021286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
